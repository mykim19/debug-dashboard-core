"""Phase 5: Code duplication checker"""

import re
from pathlib import Path

from debug_dashboard_core.scanner.base import BaseChecker, CheckResult, PhaseReport


class DuplicationChecker(BaseChecker):
    name = "duplication"
    display_name = "DUPLICATE"
    description = "Repeated URL extraction logic, yt-dlp command construction, glob patterns, bare except blocks, and duplicated DB queries."
    tooltip_why = "ì½”ë“œ ì¤‘ë³µì€ ë²„ê·¸ ìˆ˜ì • ì‹œ í•œ ê³³ë§Œ ê³ ì¹˜ê³  ë‹¤ë¥¸ ê³³ì„ ë†“ì¹˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ìœ ì§€ë³´ìˆ˜ ë¹„ìš©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤."
    tooltip_what = "URL ì¶”ì¶œ ë¡œì§ ì¤‘ë³µ(3ê°œ íŒŒì¼), yt-dlp ëª…ë ¹ì–´ ë°˜ë³µ êµ¬ì„±, glob íŒ¨í„´ ë‚¨ìš©, bare except, DB ì¿¼ë¦¬ ë°˜ë³µì„ íƒì§€í•©ë‹ˆë‹¤."
    tooltip_result = "ê²½ê³ ê°€ ì ì„ìˆ˜ë¡ ì½”ë“œë² ì´ìŠ¤ê°€ ê±´ê°•í•©ë‹ˆë‹¤. ì¤‘ë³µì´ ë§Žìœ¼ë©´ ë¦¬íŒ©í† ë§ ìš°ì„ ìˆœìœ„ê°€ ë†’ë‹¤ëŠ” ì‹ í˜¸ìž…ë‹ˆë‹¤."
    icon = "ðŸ“‹"
    color = "#f59e0b"

    def fix(self, check_name: str, project_root: Path, config: dict) -> dict:
        if check_name == "bare_except":
            phase_cfg = config.get("checks", {}).get("duplication", {})
            scan_files = phase_cfg.get("scan_files", ["app.py"])
            main_file = project_root / scan_files[0] if scan_files else None
            if not main_file or not main_file.exists():
                return {"success": False, "message": "Scan target not found"}
            src = main_file.read_text(encoding="utf-8")
            count = 0
            new_lines = []
            for line in src.splitlines(keepends=True):
                if line.strip().startswith("except:"):
                    new_lines.append(line.replace("except:", "except Exception:"))
                    count += 1
                else:
                    new_lines.append(line)
            if count > 0:
                main_file.write_text("".join(new_lines), encoding="utf-8")
                return {"success": True, "message": f"Replaced {count} bare except â†’ except Exception"}
            return {"success": True, "message": "No bare except found"}

        if check_name == "url_dup":
            # Add TODO comments to duplicated URL extraction files
            marked = 0
            for fpath in [
                project_root / "utils" / "content_hash.py",
                project_root / "mobile" / "app_mobile.py",
            ]:
                if not fpath.exists():
                    continue
                src = fpath.read_text(encoding="utf-8", errors="ignore")
                if "_normalize_youtube_url" in src and "# TODO: refactor to shared" not in src:
                    src = src.replace(
                        "def _normalize_youtube_url",
                        "# TODO: refactor to shared url_utils module\ndef _normalize_youtube_url"
                    )
                    fpath.write_text(src, encoding="utf-8")
                    marked += 1
            if marked > 0:
                return {"success": True, "message": f"Marked {marked} files with refactor TODO"}
            return {"success": True, "message": "Already marked or no duplicates found"}

        if check_name == "ytdlp_cmd":
            # Mark duplicated yt-dlp command constructions with TODO
            phase_cfg = config.get("checks", {}).get("duplication", {})
            scan_files = phase_cfg.get("scan_files", ["app.py"])
            main_file = project_root / scan_files[0] if scan_files else None
            if not main_file or not main_file.exists():
                return {"success": False, "message": "Scan target not found"}
            src = main_file.read_text(encoding="utf-8")
            lines = src.splitlines()
            new_lines = []
            count = 0
            for i, line in enumerate(lines):
                if "YT_DLP_PATH" in line and "[" in line and "# TODO: extract" not in line:
                    new_lines.append(line + "  # TODO: extract to build_ytdlp_cmd()")
                    count += 1
                else:
                    new_lines.append(line)
            if count > 0:
                main_file.write_text("\n".join(new_lines) + "\n", encoding="utf-8")
                return {"success": True, "message": f"Marked {count} yt-dlp constructions with TODO"}
            return {"success": True, "message": "Already marked or no duplicates"}

        if check_name == "file_search":
            # Mark excessive glob patterns with TODO
            phase_cfg = config.get("checks", {}).get("duplication", {})
            scan_files = phase_cfg.get("scan_files", ["app.py"])
            main_file = project_root / scan_files[0] if scan_files else None
            if not main_file or not main_file.exists():
                return {"success": False, "message": "Scan target not found"}
            src = main_file.read_text(encoding="utf-8")
            lines = src.splitlines()
            new_lines = []
            count = 0
            for line in lines:
                if (".glob(" in line or ".rglob(" in line) and "# TODO: cache" not in line:
                    new_lines.append(line + "  # TODO: cache or consolidate glob")
                    count += 1
                else:
                    new_lines.append(line)
            if count > 0:
                main_file.write_text("\n".join(new_lines) + "\n", encoding="utf-8")
                return {"success": True, "message": f"Marked {count} glob calls with TODO"}
            return {"success": True, "message": "Already marked or no excess globs"}

        if check_name == "db_query_dup":
            # Mark repeated DB queries with TODO
            phase_cfg = config.get("checks", {}).get("duplication", {})
            scan_files = phase_cfg.get("scan_files", ["app.py"])
            main_file = project_root / scan_files[0] if scan_files else None
            if not main_file or not main_file.exists():
                return {"success": False, "message": "Scan target not found"}
            src = main_file.read_text(encoding="utf-8")
            lines = src.splitlines()
            new_lines = []
            count = 0
            for line in lines:
                if "FROM videos WHERE video_id" in line and "# TODO: extract" not in line:
                    new_lines.append(line + "  # TODO: extract to get_video()")
                    count += 1
                else:
                    new_lines.append(line)
            if count > 0:
                main_file.write_text("\n".join(new_lines) + "\n", encoding="utf-8")
                return {"success": True, "message": f"Marked {count} repeated queries with TODO"}
            return {"success": True, "message": "Already marked or no duplicates"}

        return {"success": False, "message": "No auto-fix for this check"}

    def run(self, project_root: Path, config: dict) -> PhaseReport:
        report = PhaseReport(self.name)
        phase_cfg = config.get("checks", {}).get("duplication", {})
        scan_files = phase_cfg.get("scan_files", ["app.py"])

        main_file = project_root / scan_files[0] if scan_files else None
        if not main_file or not main_file.exists():
            report.add(CheckResult("main_file", CheckResult.SKIP, "No scan target"))
            return report

        src = main_file.read_text(encoding="utf-8")
        lines = src.splitlines()

        # URL extraction duplication
        url_files = []
        url_marked = 0
        for fpath, label in [
            (project_root / "app.py", "app.py"),
            (project_root / "utils" / "content_hash.py", "content_hash.py"),
            (project_root / "mobile" / "app_mobile.py", "mobile/app_mobile.py"),
        ]:
            if fpath.exists():
                s = fpath.read_text(encoding="utf-8", errors="ignore")
                if "get_video_id_from_url" in s or "_normalize_youtube_url" in s:
                    url_files.append(label)
                    if "# TODO: refactor to shared" in s:
                        url_marked += 1
        url_fix_desc = "ì¤‘ë³µ URL ì¶”ì¶œ ë¡œì§ì— ë¦¬íŒ©í† ë§ TODO ì£¼ì„ì„ ì¶”ê°€í•©ë‹ˆë‹¤ â†’ ê³µí†µ url_utils ëª¨ë“ˆë¡œ í†µí•© ê¶Œìž¥"
        if len(url_files) > 1 and url_marked == 0:
            report.add(CheckResult("url_dup", CheckResult.WARN, f"URL logic in {len(url_files)} files",
                                   details={"files": url_files}, fixable=True, fix_desc=url_fix_desc))
        elif url_marked > 0:
            report.add(CheckResult("url_dup", CheckResult.PASS, f"URL dup marked for refactor ({url_marked} files)"))
        else:
            report.add(CheckResult("url_dup", CheckResult.PASS, f"URL logic in {len(url_files)} files"))

        # yt-dlp command duplication
        cmd_lines = [i for i, l in enumerate(lines, 1) if "YT_DLP_PATH" in l and "[" in l]
        cmd_unmarked = [i for i, l in enumerate(lines, 1) if "YT_DLP_PATH" in l and "[" in l and "# TODO: extract" not in l]
        if len(cmd_unmarked) > 2:
            report.add(CheckResult("ytdlp_cmd", CheckResult.WARN, f"{len(cmd_lines)} yt-dlp constructions", fixable=True,
                                   fix_desc="ë°˜ë³µë˜ëŠ” yt-dlp ëª…ë ¹ êµ¬ì„±ì— TODO ì£¼ì„ â†’ build_ytdlp_cmd() í•¨ìˆ˜ë¡œ í†µí•© ê¶Œìž¥"))
        elif len(cmd_lines) > 2 and len(cmd_unmarked) == 0:
            report.add(CheckResult("ytdlp_cmd", CheckResult.PASS, f"yt-dlp dup marked for refactor ({len(cmd_lines)} sites)"))
        else:
            report.add(CheckResult("ytdlp_cmd", CheckResult.PASS, f"{len(cmd_lines)} yt-dlp constructions"))

        # glob pattern duplication
        glob_count = sum(1 for l in lines if ".glob(" in l or ".rglob(" in l)
        glob_unmarked = sum(1 for l in lines if (".glob(" in l or ".rglob(" in l) and "# TODO: cache" not in l)
        apple_count = sum(1 for l in lines if "startswith('._')" in l)
        if glob_unmarked > 8:
            report.add(CheckResult("file_search", CheckResult.WARN,
                                   f"{glob_count} glob calls, {apple_count} AppleDouble filters", fixable=True,
                                   fix_desc="ê³¼ë„í•œ glob/rglob í˜¸ì¶œì— ìºì‹± TODO ì£¼ì„ì„ ì¶”ê°€í•©ë‹ˆë‹¤"))
        elif glob_count > 8 and glob_unmarked == 0:
            report.add(CheckResult("file_search", CheckResult.PASS, f"glob dup marked for caching ({glob_count} calls)"))
        else:
            report.add(CheckResult("file_search", CheckResult.PASS, f"{glob_count} glob calls"))

        # bare except
        bare = [{"line": i, "code": l.strip()[:60]}
                for i, l in enumerate(lines, 1) if l.strip().startswith("except:")]
        if bare:
            report.add(CheckResult("bare_except", CheckResult.WARN, f"{len(bare)} bare except blocks",
                                   details=bare[:5], fixable=True,
                                   fix_desc="bare except: â†’ except Exception: ìœ¼ë¡œ êµì²´í•˜ì—¬ ë””ë²„ê¹… ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤"))
        else:
            report.add(CheckResult("bare_except", CheckResult.PASS, "No bare except"))

        # repeated DB queries
        db_q = sum(1 for l in lines if "FROM videos WHERE video_id" in l)
        db_unmarked = sum(1 for l in lines if "FROM videos WHERE video_id" in l and "# TODO: extract" not in l)
        if db_unmarked > 3:
            report.add(CheckResult("db_query_dup", CheckResult.WARN, f"Video query repeated {db_q} times", fixable=True,
                                   fix_desc="ë°˜ë³µ ì¿¼ë¦¬ì— TODO ì£¼ì„ â†’ get_video() í—¬í¼ í•¨ìˆ˜ë¡œ í†µí•© ê¶Œìž¥"))
        elif db_q > 3 and db_unmarked == 0:
            report.add(CheckResult("db_query_dup", CheckResult.PASS, f"DB dup marked for refactor ({db_q} queries)"))
        else:
            report.add(CheckResult("db_query_dup", CheckResult.PASS, f"Video query: {db_q} times"))

        return report
